{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "739aa096-2e37-428c-8321-1c7b746dfc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tkinter import ttk\n",
    "import pandas as pd\n",
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "def CDG(df, a, b, c):\n",
    "\n",
    "    # Diccionario de DF por cada estaciÃ³n\n",
    "    Estaciones = {}\n",
    "    percent = {}\n",
    "    # Percentilies deseados\n",
    "    Prob = [a, b, c]\n",
    "        \n",
    "    for columna in df.columns:\n",
    "        # Crear un DataFrame para cada estaciÃ³n y ordenarlo\n",
    "        est = df[[columna]].dropna().sort_values(by=columna, ascending=False).reset_index(drop=True)\n",
    "        est = est.iloc[:-3,:]\n",
    "        est = est.iloc[1:,:]\n",
    "        est['Probabilidad'] = 100*(est.index + 1) / (len(est) + 1)\n",
    "        Estaciones[columna] = est\n",
    "            \n",
    "        i = 0\n",
    "        P = pd.DataFrame(np.zeros((len(Prob), 2)), columns=['Nivel', 'Probabilidad'])\n",
    "\n",
    "        for p in Prob:\n",
    "            idx = (est.iloc[:, 1] - p).abs().idxmin()\n",
    "            P.at[i, 'Nivel'] = float(\"{:.2f}\".format(est.iloc[idx, 0]))\n",
    "            P.at[i, 'Probabilidad'] = float(\"{:.2f}\".format(est.iloc[idx, 1]))\n",
    "            i = i + 1\n",
    "        percent[columna] = P.copy()\n",
    "\n",
    "    Percentiles.append(percent)\n",
    "    \n",
    "    return Percentiles  \n",
    "\n",
    "def LeerArchivos(a):\n",
    "    # Crear una ventana raíz\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Ocultar la ventana principal\n",
    "    # Hacer que la ventana esté siempre en el frente\n",
    "    root.attributes('-topmost', True)\n",
    "    # Abrir un cuadro de diálogo para seleccionar un archivo CSV\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=f\"Selecciona un archivo CSV de {a}\",\n",
    "        filetypes=((\"CSV files\", \"*.csv\"), (\"Todos los archivos\", \"*.*\")))\n",
    "    # Mostrar la ruta del archivo seleccionado\n",
    "    print(f\"Archivo seleccionado: {file_path}\")\n",
    "\n",
    "    # Cerrar la ventana raíz\n",
    "    root.destroy()\n",
    "        \n",
    "    if file_path:  # Verificar si se seleccionó un archivo\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, index_col=None)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al leer el archivo: {e}\")\n",
    "    else:\n",
    "        print(\"No se seleccionó ningún archivo.\")\n",
    "        \n",
    "    df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "    df = df.set_index(df.columns[0])\n",
    "                \n",
    "    #df = df.iloc[:,1:] #EliminarFecahs\n",
    "        \n",
    "    return df\n",
    "\n",
    "def ElegirEstacion(Data):\n",
    "    \n",
    "    def Seleccion():\n",
    "        columna_seleccionada = columna_var.get()\n",
    "        print(f\"Has seleccionado la estación: {columna_seleccionada}\")\n",
    "        root.destroy()  # Cierra la ventana\n",
    "    \n",
    "    # Crear la ventana principal de Tkinter\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Selecciona una estación\")\n",
    "    # Hacer que la ventana esté siempre en el frente\n",
    "    root.attributes('-topmost', True)\n",
    "    # Variable para almacenar la selección\n",
    "    columna_var = tk.StringVar()\n",
    "    columna_var.set(Data.columns[0])  # Valor inicial en el OptionMenu\n",
    "    dropdown = ttk.OptionMenu(root, columna_var, *Data.columns)\n",
    "    dropdown.pack(pady=10)\n",
    "    # Botón para confirmar la selección\n",
    "    boton = tk.Button(root, text=\"Seleccionar\", command = Seleccion)\n",
    "    boton.pack(pady=20)\n",
    "    # Iniciar la ventana\n",
    "    root.mainloop()\n",
    "    columna_seleccionada = columna_var.get()\n",
    "    \n",
    "    return columna_seleccionada\n",
    "\n",
    "def ANN():\n",
    "    # Escalar los datos entre 0 y 1\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data_scaled = scaler.fit_transform(pd.DataFrame(DatosPrec[EstP]))\n",
    "    \n",
    "    # Dividir los datos en conjunto de entrenamiento y prueba (80% - 20%)\n",
    "    train_size = int(len(data_scaled) * 0.8)\n",
    "    train, test = data_scaled[:train_size], data_scaled[train_size:]\n",
    "    \n",
    "    # Crear una función para transformar los datos en formato adecuado para LSTM (multivariables)\n",
    "    def create_dataset(data, look_back=90, forecast_horizon=dias_futuros-1):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - look_back - forecast_horizon):\n",
    "            X.append(data[i:(i + look_back), :])  # Tomar todas las características\n",
    "            y.append(data[(i + look_back):(i + look_back + forecast_horizon), 0])  # Predecir la primera variable\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    # Usamos 90 días para predecir los próximos 7 días\n",
    "    look_back = 90\n",
    "    forecast_horizon = dias_futuros-1\n",
    "    X_train, y_train = create_dataset(train, look_back, forecast_horizon)\n",
    "    X_test, y_test = create_dataset(test, look_back, forecast_horizon)\n",
    "    \n",
    "    # Reshape para que el modelo LSTM lo entienda (samples, time steps, features)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
    "    \n",
    "    # Parámetros de entrenamiento\n",
    "    epochs = 20\n",
    "    batch_size = 5\n",
    "    \n",
    "    # Crear el modelo LSTM\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=(look_back, 1)))  # Hay 'num_vars' variables de entrada\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(forecast_horizon))  # Predecir los próximos 6 días\n",
    "    \n",
    "    # Compilar el modelo\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=2)\n",
    "    \n",
    "    # Realizar el pronóstico\n",
    "    test_predict = model.predict(X_test)\n",
    "    \n",
    "    # Invertir el escalado para las predicciones y los valores reales\n",
    "    test_predict_unscaled = []\n",
    "    y_test_unscaled = []\n",
    "    \n",
    "    for i in range(test_predict.shape[0]):\n",
    "        temp_predict = scaler.inverse_transform(np.concatenate([test_predict[i].reshape(-1, 1), np.tile(X_test[i, -1, 1:], (forecast_horizon, 1))], axis=1))[:, 0]\n",
    "        temp_y_test = scaler.inverse_transform(np.concatenate([y_test[i].reshape(-1, 1), np.tile(X_test[i, -1, 1:], (forecast_horizon, 1))], axis=1))[:, 0]\n",
    "        test_predict_unscaled.append(temp_predict)\n",
    "        y_test_unscaled.append(temp_y_test)\n",
    "    \n",
    "    test_predict_unscaled = np.array(test_predict_unscaled)\n",
    "    y_test_unscaled = np.array(y_test_unscaled)\n",
    "\n",
    "    return test_predict_unscaled\n",
    "\n",
    "def LogicaDifusa(P, N):\n",
    "    \n",
    "    # Antecedentes (precipitaciónn y nivel) y consecuente (Nivel o caudal)\n",
    "    PrecP = ctrl.Antecedent([P[EstP].min(), Percentiles[1][EstP].iloc[2, 0], Percentiles[1][EstP].iloc[1, 0], Percentiles[1][EstP].iloc[0, 0], P[EstP].max()], 'Precipitación Pasada')\n",
    "    PrecP2 = ctrl.Antecedent([P[EstP].min(), Percentiles[1][EstP].iloc[2, 0], Percentiles[1][EstP].iloc[1, 0], Percentiles[1][EstP].iloc[0, 0], P[EstP].max()], 'Precipitación Pasada2')\n",
    "    NivP = ctrl.Antecedent([N[EstN].min(), Percentiles[0][EstN].iloc[2,0], Percentiles[0][EstN].iloc[1,0], Percentiles[0][EstN].iloc[0, 0], N[EstN].max()], 'Nivel Pasado')\n",
    "    Niv = ctrl.Consequent([N[EstN].min(), Percentiles[0][EstN].iloc[2,0], Percentiles[0][EstN].iloc[1,0], Percentiles[0][EstN].iloc[0, 0], N[EstN].max()], 'Nivel')\n",
    "    \n",
    "    # Definición de membresí­as\n",
    "    PrecP['Bajo'] = fuzz.trimf(PrecP.universe, [P[EstP].min(), P[EstP].min(), Percentiles[1][EstP].iloc[2, 0]])\n",
    "    PrecP['Medio-Bajo'] = fuzz.trimf(PrecP.universe, [P[EstP].min(), Percentiles[1][EstP].iloc[2, 0], Percentiles[1][EstP].iloc[1, 0]])\n",
    "    PrecP['Medio'] = fuzz.trimf(PrecP.universe, [Percentiles[1][EstP].iloc[2, 0], Percentiles[1][EstP].iloc[1, 0], Percentiles[1][EstP].iloc[0, 0]])\n",
    "    PrecP['Alto'] = fuzz.trapmf(PrecP.universe, [Percentiles[1][EstP].iloc[1, 0], Percentiles[1][EstP].iloc[0, 0], P[EstP].max(), P[EstP].max()])\n",
    "\n",
    "    PrecP2['Bajo'] = fuzz.trimf(PrecP.universe, [P[EstP].min(), P[EstP].min(), Percentiles[1][EstP].iloc[2, 0]])\n",
    "    PrecP2['Medio-Bajo'] = fuzz.trimf(PrecP.universe, [P[EstP].min(), Percentiles[1][EstP].iloc[2, 0], Percentiles[1][EstP].iloc[1, 0]])\n",
    "    PrecP2['Medio'] = fuzz.trimf(PrecP.universe, [Percentiles[1][EstP].iloc[2, 0], Percentiles[1][EstP].iloc[1, 0], Percentiles[1][EstP].iloc[0, 0]])\n",
    "    PrecP2['Alto'] = fuzz.trapmf(PrecP.universe, [Percentiles[1][EstP].iloc[1, 0], Percentiles[1][EstP].iloc[0, 0], P[EstP].max(), P[EstP].max()])\n",
    "\n",
    "    NivP['Bajo'] = fuzz.trimf(Niv.universe, [N[EstN].min(), N[EstN].min(), Percentiles[0][EstN].iloc[2, 0]])\n",
    "    NivP['Medio-Bajo'] = fuzz.trimf(Niv.universe, [N[EstN].min(), Percentiles[0][EstN].iloc[2, 0], Percentiles[0][EstN].iloc[1, 0]])\n",
    "    NivP['Medio'] = fuzz.trimf(Niv.universe, [Percentiles[0][EstN].iloc[2, 0], Percentiles[0][EstN].iloc[1, 0], Percentiles[0][EstN].iloc[0, 0]])\n",
    "    NivP['Alto'] = fuzz.trapmf(Niv.universe, [Percentiles[0][EstN].iloc[1, 0], Percentiles[0][EstN].iloc[0, 0], N[EstN].max(), N[EstN].max()])\n",
    "    \n",
    "    Niv['Bajo (90-100%)'] = fuzz.trimf(Niv.universe, [N[EstN].min(), N[EstN].min(), Percentiles[0][EstN].iloc[2, 0]])\n",
    "    Niv['Medio-Bajo (50-90%)'] = fuzz.trimf(Niv.universe, [N[EstN].min(), Percentiles[0][EstN].iloc[2, 0], Percentiles[0][EstN].iloc[1, 0]])\n",
    "    Niv['Medio (10-50%)'] = fuzz.trimf(Niv.universe, [Percentiles[0][EstN].iloc[2, 0], Percentiles[0][EstN].iloc[1, 0], Percentiles[0][EstN].iloc[0, 0]])\n",
    "    Niv['Alto (0-10%)'] = fuzz.trapmf(Niv.universe, [Percentiles[0][EstN].iloc[1, 0], Percentiles[0][EstN].iloc[0, 0], N[EstN].max(), N[EstN].max()])\n",
    "\n",
    "    # REGLAS\n",
    "    B = ctrl.Rule(PrecP['Bajo'], Niv['Bajo (90-100%)'])\n",
    "    MB = ctrl.Rule(PrecP['Medio-Bajo'], Niv['Medio-Bajo (50-90%)'])\n",
    "    M = ctrl.Rule(PrecP['Medio'], Niv['Medio (10-50%)'])\n",
    "    A = ctrl.Rule(PrecP['Alto'], Niv['Alto (0-10%)'])\n",
    "\n",
    "    B2 = ctrl.Rule(PrecP2['Bajo'], Niv['Bajo (90-100%)'])\n",
    "    MB2 = ctrl.Rule(PrecP2['Medio-Bajo'], Niv['Medio-Bajo (50-90%)'])\n",
    "    M2 = ctrl.Rule(PrecP2['Medio'], Niv['Medio (10-50%)'])\n",
    "    A2 = ctrl.Rule(PrecP2['Alto'], Niv['Alto (0-10%)'])\n",
    "\n",
    "    Bn = ctrl.Rule(NivP['Bajo'], Niv['Bajo (90-100%)'])\n",
    "    MBn = ctrl.Rule(NivP['Medio-Bajo'], Niv['Medio-Bajo (50-90%)'])\n",
    "    Mn = ctrl.Rule(NivP['Medio'], Niv['Medio (10-50%)'])\n",
    "    An = ctrl.Rule(NivP['Alto'], Niv['Alto (0-10%)'])\n",
    "    \n",
    "    Nivel_ctrl = ctrl.ControlSystem([B, MB, M, A, Bn, MBn, Mn, An, B2, MB2, M2, A2]) #\n",
    "    Nivel = ctrl.ControlSystemSimulation(Nivel_ctrl)\n",
    "    \n",
    "    NR = pd.DataFrame(index=range(dias_pasados), columns=range(dias_futuros))\n",
    "\n",
    "    for i in range(dias_futuros):\n",
    "        NR.iloc[:dias_pasados-i,i] = DatosNiv[EstN].iloc[-dias_pasados+i:].values\n",
    "    \n",
    "    Pronosticos = pd.DataFrame(np.nan, index=range(dias_pasados), columns=[f\"Día {i+1}\" for i in range(dias_futuros)], dtype=float)\n",
    "\n",
    "    resultados = []\n",
    "    \n",
    "    Prec_In = pd.DataFrame(index=DatosPrec[EstP].iloc[-dias_pasados:].index, columns=range(dias_futuros))\n",
    "    Prec_In2 = pd.DataFrame(index=DatosPrec[EstP].iloc[-dias_pasados-1:-1].index, columns=range(dias_futuros))\n",
    "    \n",
    "    # Asignar valores de t-1 y t-2 (reales)\n",
    "    Prec_In[0] = DatosPrec[EstP].iloc[-dias_pasados:].values.flatten()  \n",
    "    Prec_In2[0] = DatosPrec[EstP].iloc[-dias_pasados-1:-1].values.flatten()\n",
    "    Prec_In2[1] = DatosPrec[EstP].iloc[-dias_pasados:].values.flatten()\n",
    "    \n",
    "    # Asignar predicciones a las columnas restantes\n",
    "    for i in range(dias_pasados):\n",
    "        Prec_In.iloc[i, 1:] = test_predict_unscaled[-dias_pasados+i, :]\n",
    "        Prec_In2.iloc[i, 2:] = test_predict_unscaled[-dias_pasados-1+i, 1:]\n",
    "    \n",
    "    DATOS = np.zeros((dias_pasados,4))\n",
    "    \n",
    "    for i in range(dias_pasados):\n",
    "        \n",
    "        a = dias_pasados - i\n",
    "        \n",
    "        Nivel_In = DatosNiv[EstN].iloc[[-a]].to_frame()\n",
    "        \n",
    "        for dia in range(dias_futuros):\n",
    "            \n",
    "            Nivel.input['Precipitación Pasada'] = Prec_In.iloc[i, dia]\n",
    "            Nivel.input['Precipitación Pasada2'] = Prec_In2.iloc[i, dia]\n",
    "            Nivel.input['Nivel Pasado'] = Nivel_In.iloc[dia]\n",
    "            Nivel.compute()\n",
    "            Nivel_In.loc[len(Nivel_In)] = float(Nivel.output['Nivel'])\n",
    "            Pronosticos.iloc[i, dia] = float(Nivel.output['Nivel'])\n",
    "        \n",
    "    # Calcular las métricas RMSE, MAE y R²\n",
    "    metrics = {'RMSE': [], 'MAE': [], 'R²': []}\n",
    "\n",
    "    for day in range(dias_futuros):\n",
    "\n",
    "        a = dias_pasados - day\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(NR.iloc[:a, day], Pronosticos.iloc[:a, day]))\n",
    "        mae = mean_absolute_error(NR.iloc[:a, day], Pronosticos.iloc[:a, day])\n",
    "        r2 = r2_score(NR.iloc[:a, day], Pronosticos.iloc[:a, day])\n",
    "        metrics['RMSE'].append(rmse)\n",
    "        metrics['MAE'].append(mae)\n",
    "        metrics['R²'].append(r2)\n",
    "        \n",
    "        print(f\"Día {day + 1} - RMSE: {rmse}, MAE: {mae}, R²: {r2}\")\n",
    "        \n",
    "    \n",
    "    return NR, Pronosticos, PrecP, Niv, metrics\n",
    "\n",
    "def confianza2():\n",
    "    # Definir los percentiles del eje X\n",
    "    percentiles_x = [2.5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 97.5]\n",
    "    percentiles_labels = ['P2.5', 'P10', 'P20', 'P30', 'P40', 'P50', 'P60', 'P70', 'P80', 'P90', 'P97.5']\n",
    "\n",
    "    # Obtener percentiles reales\n",
    "    real = NR.iloc[-len(Pronosticos):,0]\n",
    "    percentilesR = np.percentile(real, percentiles_x)\n",
    "\n",
    "    # Crear DataFrames para cada día y calcular percentiles y rangos en bucle\n",
    "    dias_percentiles = {}\n",
    "    for dia in range(1, dias_futuros+1):\n",
    "        # Ordenar los valores pronosticados\n",
    "        ordenados = Pronosticos.iloc[:, dia - 1].dropna().sort_values(ascending=True).reset_index(drop=True)\n",
    "\n",
    "        # Crear un DataFrame para cada día\n",
    "        dia_df = pd.DataFrame({'Nombre': percentiles_labels})\n",
    "\n",
    "        # Calcular los percentiles\n",
    "        dia_df['percentiles'] = [np.percentile(ordenados, i) for i in percentiles_x]\n",
    "\n",
    "        # Función para calcular los rangos C10, C90, C5, C95\n",
    "        def calcular_rang(ordenados, percentil, percentiles_x):\n",
    "            # Obtener el percentil anterior\n",
    "            if percentil == percentiles_x[0]:\n",
    "                p_anterior = 0\n",
    "            else:\n",
    "                p_anterior = np.percentile(ordenados, percentiles_x[percentiles_x.index(percentil) - 1])\n",
    "\n",
    "            # Filtrar los datos que están en la marca de clase\n",
    "            p = ordenados[(ordenados <= np.percentile(ordenados, percentil)) & (ordenados > p_anterior)]\n",
    "\n",
    "            # Calcular los percentiles\n",
    "            c10 = np.percentile(p, 10) if len(p) > 0 else np.nan  # Verificar longitud para evitar errores\n",
    "            c90 = np.percentile(p, 90) if len(p) > 0 else np.nan\n",
    "            c5 = np.percentile(p, 5) if len(p) > 0 else np.nan\n",
    "            c95 = np.percentile(p, 95) if len(p) > 0 else np.nan\n",
    "            return c10, c90, c5, c95\n",
    "\n",
    "        # Calcular los rangos para cada percentil y agregar al DataFrame\n",
    "        dia_df[['C10', 'C90', 'C5', 'C95']] = [calcular_rang(ordenados, i, percentiles_x) for i in percentiles_x]\n",
    "\n",
    "        # Guardar el DataFrame del día en el diccionario\n",
    "        dias_percentiles[f'Dia_{dia}'] = dia_df\n",
    "\n",
    "    return dias_percentiles, percentilesR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f84cdee-5e3c-4476-883d-8fbf541cba84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo seleccionado: C:/Users/juanj/Downloads/Estaciones/Estaciones/H12/Seleccionado/Niveles_H12_relleno.csv\n",
      "Archivo seleccionado: C:/Users/juanj/Downloads/Estaciones/Estaciones/H12/Seleccionado/Niveles_H12_relleno.csv\n",
      "Has seleccionado la estación: H12\n",
      "Archivo seleccionado: C:/Users/juanj/Downloads/Estaciones/Estaciones/H12/NetCDF/ATP01PT02.csv\n",
      "Has seleccionado la estación: ATP01PT02\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ingrese el número de días pasados:  307\n",
      "Ingrese el número de días futuros:  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "304/304 - 20s - loss: 0.0590 - val_loss: 0.0534 - 20s/epoch - 65ms/step\n",
      "Epoch 2/20\n",
      "304/304 - 13s - loss: 0.0586 - val_loss: 0.0536 - 13s/epoch - 41ms/step\n",
      "Epoch 3/20\n",
      "304/304 - 12s - loss: 0.0583 - val_loss: 0.0530 - 12s/epoch - 41ms/step\n",
      "Epoch 4/20\n",
      "304/304 - 13s - loss: 0.0581 - val_loss: 0.0514 - 13s/epoch - 41ms/step\n",
      "Epoch 5/20\n",
      "304/304 - 12s - loss: 0.0579 - val_loss: 0.0523 - 12s/epoch - 41ms/step\n",
      "Epoch 6/20\n",
      "304/304 - 12s - loss: 0.0578 - val_loss: 0.0518 - 12s/epoch - 41ms/step\n",
      "Epoch 7/20\n",
      "304/304 - 12s - loss: 0.0577 - val_loss: 0.0510 - 12s/epoch - 41ms/step\n",
      "Epoch 8/20\n",
      "304/304 - 12s - loss: 0.0577 - val_loss: 0.0512 - 12s/epoch - 40ms/step\n",
      "Epoch 9/20\n",
      "304/304 - 13s - loss: 0.0575 - val_loss: 0.0499 - 13s/epoch - 41ms/step\n",
      "Epoch 10/20\n",
      "304/304 - 12s - loss: 0.0576 - val_loss: 0.0516 - 12s/epoch - 41ms/step\n",
      "Epoch 11/20\n",
      "304/304 - 12s - loss: 0.0576 - val_loss: 0.0525 - 12s/epoch - 41ms/step\n",
      "Epoch 12/20\n",
      "304/304 - 12s - loss: 0.0575 - val_loss: 0.0509 - 12s/epoch - 40ms/step\n",
      "Epoch 13/20\n",
      "304/304 - 12s - loss: 0.0574 - val_loss: 0.0520 - 12s/epoch - 41ms/step\n",
      "Epoch 14/20\n",
      "304/304 - 13s - loss: 0.0576 - val_loss: 0.0504 - 13s/epoch - 41ms/step\n",
      "Epoch 15/20\n",
      "304/304 - 13s - loss: 0.0574 - val_loss: 0.0500 - 13s/epoch - 41ms/step\n",
      "Epoch 16/20\n",
      "304/304 - 12s - loss: 0.0574 - val_loss: 0.0502 - 12s/epoch - 41ms/step\n",
      "Epoch 17/20\n",
      "304/304 - 12s - loss: 0.0574 - val_loss: 0.0507 - 12s/epoch - 40ms/step\n",
      "Epoch 18/20\n",
      "304/304 - 12s - loss: 0.0573 - val_loss: 0.0510 - 12s/epoch - 41ms/step\n",
      "Epoch 19/20\n",
      "304/304 - 13s - loss: 0.0573 - val_loss: 0.0501 - 13s/epoch - 41ms/step\n",
      "Epoch 20/20\n",
      "304/304 - 12s - loss: 0.0573 - val_loss: 0.0505 - 12s/epoch - 41ms/step\n",
      "10/10 [==============================] - 2s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "Datos_Historicos_Nivel = LeerArchivos(\"datos historicos de nivel\")\n",
    "DatosNiv = LeerArchivos(\"periodo seleccionado de nivel\")\n",
    "EstN = ElegirEstacion(DatosNiv)\n",
    "DatosPrec = LeerArchivos(\"periodo seleccionado de precipitacion\")\n",
    "EstP = ElegirEstacion(DatosPrec)\n",
    "\n",
    "dias_pasados = int(input(\"Ingrese el número de días pasados: \"))\n",
    "dias_futuros = int(input(\"Ingrese el número de días futuros: \"))\n",
    "\n",
    "test_predict_unscaled = ANN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62c688e6-c462-4d27-b237-35c9578abebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Día 1 - RMSE: 11.485588443675885, MAE: 9.868328564922706, R²: -3.8405184009109155\n",
      "Día 2 - RMSE: 19.259018264604748, MAE: 17.9506206162923, R²: -12.573011646227663\n",
      "Día 3 - RMSE: 23.564223996475288, MAE: 22.87197095312285, R²: -19.286041298433556\n",
      "Día 4 - RMSE: 24.931170179165427, MAE: 24.382829208174066, R²: -21.678950700648787\n",
      "Día 5 - RMSE: 25.005699479954625, MAE: 24.45153257118151, R²: -21.80220814257054\n",
      "Día 6 - RMSE: 24.97358979016705, MAE: 24.418480036006308, R²: -21.725371303324593\n",
      "Día 7 - RMSE: 24.93275541705994, MAE: 24.376216810990314, R²: -21.62426863035763\n"
     ]
    }
   ],
   "source": [
    "Percentiles = []\n",
    "Percentiles = CDG(DatosNiv, 10, 50, 90)\n",
    "Percentiles = CDG(DatosPrec, 0, 80, 90)\n",
    "\n",
    "NR, Pronosticos, PrecP, Niv, metrics = LogicaDifusa(DatosPrec, DatosNiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0546b2a-2d21-4dd2-a7be-e91b553fb009",
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_percentiles, percentilesR = confianza2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d156b3-ea9c-4045-a923-2433fa146105",
   "metadata": {},
   "source": [
    "# Generar PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "9168a242-6cdf-4d05-a421-c88d252d1589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados y gráficos guardados en el archivo NetCDF_Precipitacion_Ponderada_2_Variables_P(t-1)_N(t-1).pdf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juanj\\.conda\\envs\\Scikit-fuzzi\\lib\\site-packages\\matplotlib\\figure.py:456: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  warnings.warn(\n",
      "C:\\Users\\juanj\\.conda\\envs\\Scikit-fuzzi\\lib\\site-packages\\matplotlib\\figure.py:456: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generar nombre del archivo PDF dinámico basado en variables y nombres de las columnas\n",
    "pdf_filename = f'NetCDF_{EstP}_2_Variables_P(t-1)_N(t-1).pdf'\n",
    "\n",
    "# Crear el archivo PDF para guardar gráficos y métricas\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    # Página 1: Información general\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.text(0.5, 0.9, 'Variables Utilizadas en el Pronóstico:', ha='center', fontsize=14)\n",
    "    plt.text(0.5, 0.8, f'{EstP}, {EstN}', ha='center', fontsize=12)\n",
    "    plt.text(0.5, 0.7, f'Variable Pronosticada: Caudal en estación {EstN}', ha='center', fontsize=14)\n",
    "    plt.text(0.5, 0.6, 'Reglas basadas en precipitación t-1 y nivel t-1', ha='center', fontsize=14)\n",
    "    plt.text(0.5, 0.5, 'Rangos de: Nivel(10, 50, 90), Precipitación(0, 80, 90)', ha='center', fontsize=12)\n",
    "    plt.text(0.5, 0.4, 'Métricas de Ajuste:', ha='center', fontsize=14)\n",
    "    for i in range(dias_futuros):\n",
    "        plt.text(0.5, 0.3 - i*0.05, f\"Día {i + 1} - RMSE: {metrics['RMSE'][i]:.4f}, MAE: {metrics['MAE'][i]:.4f}, R²: {metrics['R²'][i]:.4f}\",\n",
    "                 ha='center', fontsize=12)\n",
    "    plt.axis('off')\n",
    "    pdf.savefig()\n",
    "    plt.close()\n",
    "    \n",
    "    # Gráficos de predicción e intervalos de confianza en una fila para cada día\n",
    "    for day in range(dias_futuros):\n",
    "        \n",
    "        a = dias_pasados - day\n",
    "        Dia = dias_percentiles[f'Dia_{day+1}']\n",
    "        \n",
    "        # Crear subplots: 1 fila y 2 columnas\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Gráfico 1: Pronóstico vs Realidad\n",
    "        ax1.set_title(f'Pronóstico del Día {day+1}')\n",
    "        ax1.plot(DatosPrec.index[-len(NR)+day:], NR.iloc[:a, day], label= 'Nivel Real')\n",
    "        ax1.plot(DatosPrec.index[-len(NR):], Pronosticos.iloc[:, day], label= 'Pronóstico', color='red')\n",
    "        ax1.set_xlabel('Fecha')\n",
    "        ax1.set_ylabel('nivel (m)')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "    \n",
    "        # Gráfico 2: Intervalos de confianza\n",
    "        ax2.set_title(f'Día {day+1} - Intervalo de Confianza')\n",
    "        ax2.plot(Dia['Nombre'], percentilesR, label='Nivel real', color='black', linewidth=1.5)\n",
    "    \n",
    "        # Graficar los intervalos de confianza\n",
    "        ax2.fill_between(Dia['Nombre'], Dia['C10'], Dia['C90'], color='blue', alpha=0.2, label='C10-C90')\n",
    "        ax2.fill_between(Dia['Nombre'], Dia['C5'], Dia['C95'], color='gray', alpha=0.2, label='C5-C95')\n",
    "    \n",
    "        ax2.set_xlabel('Percentil')\n",
    "        ax2.set_ylabel('Nivel (m)')\n",
    "        ax2.legend(loc='upper left')\n",
    "        ax2.grid(True)\n",
    "    \n",
    "        # Ajustar el espaciado entre los subplots\n",
    "        plt.tight_layout()\n",
    "\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "        \n",
    "    # Gráfico de todas las métricas por día\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    days = range(1, 8)\n",
    "    plt.plot(days, metrics['RMSE'], label='RMSE')\n",
    "    plt.plot(days, metrics['MAE'], label='MAE')\n",
    "    plt.plot(days, metrics['R²'], label='R²')\n",
    "    plt.title('Métricas de Pronóstico para Cada Día')\n",
    "    plt.xlabel('Día de Pronóstico')\n",
    "    plt.ylabel('Métrica')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    pdf.savefig()\n",
    "    plt.close()\n",
    "    \n",
    "    # Gráfico membresías de precipitación y nivel\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    Niv.view()\n",
    "    pdf.savefig()\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    PrecP.view()\n",
    "    pdf.savefig()\n",
    "    plt.close()\n",
    "    \n",
    "# Mensaje de finalización\n",
    "print(f\"Resultados y gráficos guardados en el archivo {pdf_filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c6d88-83d3-4c1c-b08a-116cadeb2f26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
